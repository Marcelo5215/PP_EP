\documentclass[12pt]{article}

\usepackage{sbc-template}

\usepackage{graphicx,url}

\usepackage{amsmath}

%\usepackage[brazilian]{babel}   
\usepackage[latin1]{inputenc}  

     
\sloppy

\title{Product Quantization for Nearest Neighbor Search\\ A parallel aproach}

\author{Marcelo de Araújo \inst{1}, André Fernandes \inst{1} }


\address{Departamento de Ciência da Computação - Universidade de Brasília(UNB)
}

\begin{document} 

\maketitle
     
\begin{resumo} 
 O artigo baseia-se na ideia proposta por \cite{pqnns:1},  onde o espaço é decomposto em vários subespaços de um produto cartesiano, produzindo vetores menores, que serão aproximados separadamente, e usados para a criação de uma lista invertida junto com uma base de dados contendo os códigos referentes a cada vetor da base, onde toda busca será feita por meio da lista invertida.
 Também será apresentada uma proposta de paralelização no ambiente distribuído, com o foco na parte de busca.
\end{resumo}


\section{Introdução}\label{sec:Intro}
    Dados um vetor $x$, e um conjunto de vetores $Y \subset R^n$, queremos achar o vetor $y$ do conjunto $Y$ que mais se aproxima de $x$, chamando de $NN(x)$ o vizinho mais próximo e definido como:
    \begin{equation}\label{NNdef}
	    NN(x) = \arg \min d(x, y) \ , \ y \in Y
    \end{equation}
    \begin{equation}\label{Ddef}
	    d(x, y) = \sqrt{\sum_{i=1}^{n}(x_i-y_i)^2}
    \end{equation}
	Onde $d(x, y)$ é a distância euclidiana entre $x$ e $y$.
	Porém para conjuntos $Y$ grandes seria muito custoso a busca exaustiva. Por isso a estratégia adotada em [1], tenta aproximar os vetores da base $Y$ em outro conjunto de vetores, chamados centróides($c_i \in C$) aproximados com o algoritmo K-means a partir de um conjunto de treino.\\
	Com o centróides conhecidos podemos definir formalmente como $q(.)$ a função que mapeia um vetor arbitrário $x \in R^n$ em $q(x) \in C = \{c_i \ ; \ i \in I\}$, onde I é um intervalo finito, $I = \{0, \cdots ,k-1\}$ e $c_i$ são centróídes.
	\begin{equation}\label{Qdef}
	    q(x) = \arg \min d(x, c_i) \ , \ c_i \in C
	\end{equation}
	
	\begin{figure}[h]
		\centering
		\includegraphics[scale=1.1]{fig1}
		\label{fig1centroids}
		\caption{Centroides e Vetores}
	\end{figure}
	Além de aproximar os vetores $y$ da base em seus centróides mais próximos, centróides são criados a partir de subvetores, e assim vetores $y$ são divididos em partes de dimensão $d = \frac{n}{m}$ e assinalada a cada subdimensão do centróide.
	\begin{equation}\label{eq:u}
	\begin{split}
		y = \{y_1, y_1 , \cdots , y_n\},\ \text{seus respectivos subvetores $u_i$}\\
		u_1 = \{y_1, y_2, \cdots, y_d\}  , \ u_2 = \{y_{d+1}, y_{d+2}, \cdots, y_{2d}\}\\
		u_{m}= \{y_{n-d}, y_{n-d+1}, \cdots, y_{n}\}, \ u_i \in R^{d} \qquad
	\end{split}
	\end{equation}
	
	E seus respectivos centróídes de seus subespaços:
	\begin{equation}\label{eq:qy}
		q(y) = \{ q(u_1), q(u_2), \cdots, q(u_m)\}, \ q(u_i) \in C
	\end{equation}
	
	\subsection{Lista Invertida}
		Com a finalidade de tornar a busca mais eficiente uma estrutura de lista invertida foi utilizada
		por \cite{pqnns:1}.\\
		Para montar a lista são usados dois conjuntos de centróides $C_1$ e $C_2$, onde $C_1$ representa os centroides assinalados a base de treino $T$, chamados em \cite{pqnns:1} por \textit{coarse centroids}, e após conhecidos, $C_2$ é calculado e são os centróides assinalados ao resto , $r(t)$, dos vetores de treino com cada um de seus centróides.
		
		\begin{equation}\label{eq:ind}
			\begin{split}
				q(t) \in C_1 \qquad \quad \\
				r(t) = y - q(t), \ y \in T \\
				q(r(t)) \in C_2 \qquad
			\end{split}
		\end{equation}
		
		Com os conjuntos $C_1$ e $C_2$ conhecidos, podemos montar a estrutura da lista em si, indexando os vetores de uma base $Y$ na lista, da seguinte forma:
		
			\begin{figure}[h]
				\centering
				\includegraphics[scale=0.5]{indexing}
				\caption{Processo de indexação}
				\label{fig:fig2indexing}
			\end{figure}
		
		Cada entrada da lista representa um centróide de $C_1$ e cada entrada da lista contida representa o centroide de $C_2$ possuindo os identificadores dos vetores $y$ da base que possuem aquele centróide como o mais próximo.\\
		
\section{Algoritmo} \label{sec:Algoritmo}
	\subsection{Aprendizagem}
	Primeiramente o algoritmo necessita aprender os centróides $c_i$ dos dois conjuntos $C_1$ e $C_2$, para sabermos a função $q(.)$, e realiza isto na parte de aprendizagem, onde a partir de uma base de treino $T$ os conjuntos são aprendidos com o algoritmo K-means.
	
	\subsection{Indexação}
		A figura \ref{fig:fig2indexing} representa o processo de indexação de uma base de dados $Y$, onde e feito da seguinte forma:
		\begin{itemize}
			\item Para cada vetor $y_i \in Y$ calculamos seu centroide mais próximo $c_i \in C_1$, assim sabemos a entrada da lista principal.
			\item Calculamos $r(y_i)$ conforme \eqref{eq:ind} e calculamos o centroide mais próximo $q(r(y_i)) = c_j \in C_2$, para cada subdimensão
			\item Agora que temos o código para cada $c_j \in C_2$, guardamos na entrada correspondente junto com o identificador do vetor. 
		\end{itemize}

	\subsection{Busca}
	Durante a busca, como dito na secção \ref{sec:Intro}, queremos buscar o vizinho mais próximo de um determinado vetor $x$, ou $k$ vizinhos mais próximos dele.
	
	\begin{itemize}
		\item Procuramos o centroide $c_i \in C_1$ mais próximo de $x$, agora sabemos qual entrada da lista possui vetores associados ao mesmo centroide.
		\item Calculamos o $r(x)$ e usamos para calcular a $d(r(x), c_j), \ c_j \in C_2$, para cada subdimensão.
		\item Somamos as distâncias das subdimensões de interesse, aquelas cujos $c_j$ se encontram na entrada da listad descoberta no primeiro passo.
		\item Com as distâncias podemos procurar as $k$ distância mínimas, gerando uma lista $L$ a de possíveis canditados da base $Y$ próximos a $x$, que são 
		encontrados pelos seus identificadores presentes nas entradas de cada lista. 
	\end{itemize}
	
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.5]{search}
		\label{fig3search}
		\caption{Processo de Busca}
	\end{figure}
	
	 
	 
\section{Solução Paralela}


\section{Resultados}


\section{Conclusão}\label{sec:conc}


\bibliographystyle{sbc}
\bibliography{sbc-template}
\cite{pqnns:1}

\end{document}
